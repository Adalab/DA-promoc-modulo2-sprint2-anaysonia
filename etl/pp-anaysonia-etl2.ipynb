{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios Pair Programming - Modulo 2 - Sprint 2 \n",
    "## Lecci贸n 2 - ETL II: Transformaci贸n I, Limpieza\n",
    "### Ana Gonzalez y Sonia Ruiz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendr茅is que usar el csv attacks_limpieza_completa que ten茅is adjunto abajo.\n",
    "\n",
    "En la lecci贸n de hoy aprendimos como transformar nuestros datos para que est茅n preparados para almacearlos en una BBDD. En este momento tenemos dos fuentes de datos:\n",
    "1. El csv con los ataques de tiburones que hemos estado limpiando hasta ahora, el que os hemos adjuntado *(attacks_limpieza_completa)*. Sentiros libres de usar vuestros propios csv en caso de que quer谩is.\n",
    "2. El csv con los datos clim谩ticos de los principales paises que tienen ataques de tiburones, el que creamos en el pair programming de ayer.\n",
    "El objetivo de la sesi贸n de hoy ser谩 juntar en un 煤nico csv la informaci贸n de ambas fuentes. Para ello:\n",
    "- Cargaremos los dos ficheros de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Del dataframe de los ataques nos quedaremos solo con las filas de los pa铆ses que seleccionamos en la lecci贸n de ayer:\n",
    "    - USA\n",
    "    - Australia\n",
    "    - New Zealand\n",
    "    - South Africa\n",
    "    - Papua New Guinea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Del dataframe de los datos clim谩ticos seleccionaremos todas las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cuando ya tengamos todos los datos deseados juntaremos los dos csv.\n",
    "    - Para hacer esta uni贸n tendremos que hacer un groupby en la tabla de clima para sacar una media de las medidas clim谩ticas por pa铆s.\n",
    "    - Antes de hacer el groupby si nos fijamos tenemos dos columnas *rh_profile* y *wind_profile* cuya informaci贸n es una lista de diccionarios. Si intentamos hacer la media de eso no nos dar谩 un valor real. A este problema ya nos enfrentamos en la clase invertida de ETL-2, donde ten铆ais un Bonus para desempaquetar esta informaci贸n. En caso de que en aquel ejercicio no lo consigierais os dejamos por aqu铆 una posible soluci贸n que nos permite separar esa informaci贸n en distintas columnas. Os dejamos el c贸digo documentado. 锔 Os recomendamos que vay谩is desgranando el c贸digo y viendo lo que nos devuelve cada l铆nea de c贸digo para entenderlo mejor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````python\n",
    "# os recomendamos resetear el index del dataframe de los datos clim谩ticos para que no se repitan los nombres de las columnas.\n",
    "\n",
    "\n",
    "# El primer problema al que nos podemos enfrentar es que si vemos los tipos de las columnas vemos que estas columnas son objetos, es decir, strings, lo que har谩 que trabajar con ellas sea un poco complicado: \n",
    "clima.dtypes\n",
    "\n",
    "timepoint             int64\n",
    "cloudcover            int64\n",
    "highcloud             int64\n",
    "midcloud              int64\n",
    "lowcloud              int64\n",
    "rh_profile           object\n",
    "wind_profile         object\n",
    "\n",
    "# en Python tenemos la librer铆a `ast` que nos permite castear un string que dentro tiene diccionarios, o listas o tuplas a su tipo correspondiente. En nuestro caso, lo que conseguiremos es no tener strings sino listas en la columna. Esto lo haremos de la siguiente forma: \n",
    "\n",
    "import ast\n",
    "\n",
    "clima['wind_profile']= clima['wind_profile'].apply(ast.literal_eval)\n",
    "\n",
    "# una vez que tengamos la columna cambiada, una fantas铆a de Pandas es que si hago un apply sobre una columna cuyos valores son diccionarios o listas nos va a genererar una columna con los valores de los diccionarios o listas. Donde cada columna ser谩 key del diccionario o cada elemento de la lista. \n",
    "\n",
    "\n",
    "x = clima['wind_profile'].apply(pd.Series)\n",
    "\n",
    "\n",
    "# nos creamos un dataframe nuevo con el resultado de la informaci贸n de una de las columnas separadas por columnas. Esto nos va a devolver un dataframe donde cada fila ser谩 una celda del dataframe anterior. \n",
    "x = df['rh_profile'].apply(pd.Series) \n",
    "\n",
    "# 驴Qu茅 es lo que ocurre cuando hacemos esto?\n",
    "# Nos ha creado tantas columnas como valores tuvieramos en la lista. Donde columna es, en este caso, un diccionario (porque nuestra lista esta compuesta por distintos diccionarios)\n",
    "\n",
    "# Ok, hemos conseguido desempaquetar la informaci贸n de la lista en distintas columnas. Ahora tenemos que despempaquetar la informaci贸n de los diccionarios en distintas columnas. En este caso, lo que querremos es que las key de los diccionarios sean los nombres de las columnas y los values los valores de las celdas del dataframe. Volveremos a seguir entonces la misma l贸gica que antes con el apply, pero en este caso necesitamos hacerlo para todo el dataframe (que es x): \n",
    "\n",
    "# Por eso empezamos con un for para iterar por cada una de las columnas. \n",
    "for i in range(len(x.columns)): \n",
    "\n",
    "    # aplicamos el apply,extraemos el valore de la key \"layer\" y lo almacenamos en una variable que convertimos a string \n",
    "    nombre = \"rh_\" + str(x[i].apply(pd.Series)[\"layer\"][0]) \n",
    "\n",
    "    # hacemos lo mismo con una variable que se llame valores para \"guardar\" los valores de la celda\n",
    "    valores = list(x[i].apply(pd.Series)[\"rh\"] )\n",
    "\n",
    "    # usamos el m茅todo insert de los dataframes para ir a帽adiendo esta informaci贸n a el dataframe con la informaci贸n del clima. \n",
    "    df.insert(i, nombre, valores)\n",
    "\n",
    "# una vez que hayamos hecho esto para las dos columnas ya podremos hacer el gropuby para despu茅s unir toda la informaci贸n. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Guardar los resultados obtenidos en un csv que usaremos en pr贸ximos ejercicios de pair programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Happy coding** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 13:09:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "468215beec16e96f99bc366ecab75b1d75db716e353e0417cbdb37be88f0d883"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
